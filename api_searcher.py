import aiohttp
import asyncio
import os
import re
import requests
from bs4 import BeautifulSoup
from googletrans import Translator
from googlesearch import search as google_search

TMDB_API_KEY = os.getenv("TMDB_API")
translator = Translator()


async def scrape_google_search_info(query, content_type="ë§Œí™”"):
    """Google ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì œëª©, ì‘ê°€, ì—°ë„ ì •ë³´ ì¶”ì¶œ"""
    try:
        await asyncio.sleep(1)  # rate limit ë°©ì§€
        loop = asyncio.get_event_loop()

        def _scrape():
            search_query = f"{query} {content_type}"
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            }

            try:
                # Google ê²€ìƒ‰
                urls = list(google_search(search_query, num_results=5, lang='ko'))

                for url in urls:
                    try:
                        response = requests.get(url, headers=headers, timeout=5)
                        if response.status_code != 200:
                            continue

                        soup = BeautifulSoup(response.text, 'html.parser')

                        # ë‚˜ë¬´ìœ„í‚¤ì—ì„œ ì¶”ì¶œ
                        if 'namu.wiki' in url:
                            title = soup.find('h1', {'class': 'wiki-title'})
                            if title:
                                title_text = title.get_text(strip=True)

                                # ì‘ê°€/ì €ì ì •ë³´ ì¶”ì¶œ
                                author = None
                                author_patterns = ['ì €ì', 'ì‘ê°€', 'ì›ì‘', 'ë§Œí™”ê°€']
                                for pattern in author_patterns:
                                    author_elem = soup.find(string=re.compile(f'{pattern}|{pattern}', re.IGNORECASE))
                                    if author_elem:
                                        # ë‹¤ìŒ í…ìŠ¤íŠ¸ ë…¸ë“œê°€ ì‘ê°€ëª…
                                        parent = author_elem.parent
                                        if parent and parent.next_sibling:
                                            author = parent.next_sibling.get_text(strip=True)
                                            break

                                # ì—°ë„ ì¶”ì¶œ
                                year = None
                                year_match = re.search(r'(20\d{2})', response.text)
                                if year_match:
                                    year = year_match.group(1)

                                if title_text:
                                    return {
                                        'title': title_text,
                                        'author': author or "ì •ë³´ ì—†ìŒ",
                                        'year': year,
                                        'img_url': None
                                    }

                        # Wikipediaì—ì„œ ì¶”ì¶œ
                        elif 'wikipedia' in url:
                            title = soup.find('h1', {'class': 'firstHeading'})
                            if title:
                                title_text = title.get_text(strip=True)

                                # ì •ë³´ìƒì(infobox)ì—ì„œ ì‘ê°€ ì¶”ì¶œ
                                author = None
                                infobox = soup.find('table', {'class': 'infobox'})
                                if infobox:
                                    rows = infobox.find_all('tr')
                                    for i, row in enumerate(rows):
                                        if re.search(r'ì €ì|ì‘ê°€|ì›ì‘|Author', row.get_text(), re.IGNORECASE):
                                            if i + 1 < len(rows):
                                                author = rows[i + 1].get_text(strip=True)
                                                break

                                year = None
                                year_match = re.search(r'(20\d{2})', response.text)
                                if year_match:
                                    year = year_match.group(1)

                                if title_text:
                                    return {
                                        'title': title_text,
                                        'author': author or "ì •ë³´ ì—†ìŒ",
                                        'year': year,
                                        'img_url': None
                                    }

                    except Exception as e:
                        print(f"âš ï¸ URL íŒŒì‹± ì‹¤íŒ¨ ({url}): {e}")
                        continue

                return None

            except Exception as e:
                print(f"âŒ Google ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                return None

        result = await loop.run_in_executor(None, _scrape)
        return result

    except Exception as e:
        print(f"âŒ Google ìŠ¤í¬ë˜í•‘ ì—ëŸ¬: {e}")
        return None


async def is_korean(text):
    """í•œê¸€ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
    if not text:
        return False
    return bool(re.search('[ê°€-í£]', text))


async def translate_to_korean(text):
    """ì˜ì–´/ì¼ë³¸ì–´ ì´ë¦„ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­"""
    if not text or text == "N/A" or await is_korean(text):
        return text

    try:
        result = await translator.translate(text, dest='ko')
        return result.text
    except Exception as e:
        print(f"Google Translate failed: {e}")
        # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
        return text
    
async def translate_to_english(text):
    if not text or text == "N/A":
        return text

    try:
        result = await translator.translate(text, dest='en')
        return result.text
    except Exception as e:
        print(f"Google Translate failed: {e}")
        # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
        return text

class ContentSearcher:
    """ì˜í™”, ë§Œí™”, ì›¹íˆ° ê²€ìƒ‰ í†µí•© í´ë˜ìŠ¤"""

    @staticmethod
    async def _search_tmdb_direct(session, name):
        """TMDBì—ì„œ ì§ì ‘ ê²€ìƒ‰ (ë‚´ë¶€ìš©)"""
        search_url = f"https://api.themoviedb.org/3/search/multi?api_key={TMDB_API_KEY}&query={name}&language=ko-KR"
        async with session.get(search_url) as response:
            data = await response.json()

        if data.get('results'):
            results = [r for r in data['results'] if r.get('media_type') in ('movie', 'tv')]
            if not results:
                return name, "N/A", "N/A", None, "movie"

            item = results[0]
            media_type = item.get('media_type')
            genre_ids = item.get('genre_ids', [])

            is_animation = 16 in genre_ids
            if is_animation:
                category = 'anime'
            elif media_type == 'tv':
                category = 'drama'
            else:
                category = 'movie'

            if media_type == 'movie':
                title = item.get('title', name)
                year = item['release_date'][:4] if item.get('release_date') else "N/A"
            else:
                title = item.get('name', name)
                year = item['first_air_date'][:4] if item.get('first_air_date') else "N/A"

            item_id = item['id']
            director = None

            if not await is_korean(title):
                title = await translate_to_korean(title)

            if media_type == 'movie':
                credits_url = f"https://api.themoviedb.org/3/movie/{item_id}/credits?api_key={TMDB_API_KEY}&language=ko-KR"
                async with session.get(credits_url) as credits_response:
                    credits = await credits_response.json()
                director_info = next((crew for crew in credits.get('crew', []) if crew['job'] == 'Director'), None)

                if director_info:
                    director = director_info.get('name')
                    if not await is_korean(director):
                        director = await translate_to_korean(director)
            else:
                details_url = f"https://api.themoviedb.org/3/tv/{item_id}?api_key={TMDB_API_KEY}&language=ko-KR"
                async with session.get(details_url) as details_response:
                    details = await details_response.json()
                creators = details.get('created_by', [])
                if creators:
                    director = creators[0]['name']
                    if not await is_korean(director):
                        director = await translate_to_korean(director)

            poster_path = item.get('poster_path')
            img_url = f"https://image.tmdb.org/t/p/w500{poster_path}" if poster_path else None

            return title, year, director, img_url, category

        return None, None, None, None, None

    @staticmethod
    async def search_tmdb(session, name):
        """TMDB multi search APIë¡œ ì˜í™”/ë“œë¼ë§ˆ/ì• ë‹ˆ ê²€ìƒ‰ ë° ìë™ ë¶„ë¥˜ (Google fallback í¬í•¨)"""
        # 1ì°¨: ì§ì ‘ ê²€ìƒ‰
        result = await ContentSearcher._search_tmdb_direct(session, name)

        # ê²€ìƒ‰ ì„±ê³µ ì‹œ ë°˜í™˜
        if result[0] is not None:
            return result

        # 2ì°¨: Google ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ ì •ë³´ ì¶”ì¶œ
        print(f"ğŸ” TMDB ì§ì ‘ ê²€ìƒ‰ ì‹¤íŒ¨, Google ìŠ¤í¬ë˜í•‘ ì‹œë„: {name}")
        google_info = await scrape_google_search_info(name, "ì˜í™”")

        if google_info and google_info.get('title'):
            print(f"ğŸ” Googleì—ì„œ ì¶”ì¶œí•œ ì •ë³´ - ì œëª©: {google_info['title']}, ê°ë…: {google_info.get('author')}")
            return google_info['title'], google_info.get('year'), google_info.get('author'), None, 'movie'

        # 3ì°¨: ë²ˆì—­ í›„ ì¬ê²€ìƒ‰
        translated = await translate_to_english(name)
        if translated and translated != name:
            print(f"ğŸ” ë²ˆì—­ëœ ì œëª©ìœ¼ë¡œ TMDB ì¬ê²€ìƒ‰: {translated}")
            result = await ContentSearcher._search_tmdb_direct(session, translated)
            if result[0] is not None:
                return result

        return None, None, None, None, None

    @staticmethod
    async def _search_manga_direct(session, name):
        """MangaDexì—ì„œ ì§ì ‘ ê²€ìƒ‰ (ë‚´ë¶€ìš©)"""
        url = f"https://api.mangadex.org/manga?title={name}&limit=1&includes[]=author&includes[]=cover_art"

        try:
            async with session.get(url) as response:
                data = await response.json()

            if data.get('data') and len(data['data']) > 0:
                manga = data['data'][0]
                attributes = manga.get('attributes', {})
                title_dict = attributes.get('title', {})
                alt_titles = attributes.get('altTitles', [])

                title = None
                # 1. title ê°ì²´ì—ì„œ í•œêµ­ì–´ ì œëª© ì°¾ê¸°
                if 'ko' in title_dict:
                    title = title_dict['ko']

                # 2. altTitlesì—ì„œ í•œêµ­ì–´ ì œëª© ì°¾ê¸°
                if not title:
                    for alt in alt_titles:
                        if 'ko' in alt:
                            title = alt['ko']
                            break

                # 3. title ê°ì²´ì—ì„œ ì˜ì–´ ì œëª© ì°¾ê¸°
                if not title and 'en' in title_dict:
                    title = title_dict['en']

                # 4. ê·¸ë˜ë„ ì—†ìœ¼ë©´ ì²«ë²ˆì§¸ ì œëª© ì‚¬ìš©
                if not title:
                    title = list(title_dict.values())[0] if title_dict else name

                year = str(attributes.get('year')) if attributes.get('year') else None
                author = None
                relationships = manga.get('relationships', [])
                for rel in relationships:
                    if rel.get('type') == 'author':
                        author_attrs = rel.get('attributes', {})
                        author = author_attrs.get('name')
                        if author and not await is_korean(author):
                            author = await translate_to_korean(author)
                        break

                img_url = None
                for rel in relationships:
                    if rel.get('type') == 'cover_art':
                        cover_attrs = rel.get('attributes', {})
                        filename = cover_attrs.get('fileName')
                        if filename:
                            manga_id = manga.get('id')
                            img_url = f"https://uploads.mangadex.org/covers/{manga_id}/{filename}"
                        break

                return title, year, author, img_url

        except Exception as e:
            print(f"âŒ MangaDex API error: {e}")

        return None, None, None, None

    @staticmethod
    async def search_manga(session, name):
        """MangaDexì—ì„œ ë§Œí™” ê²€ìƒ‰ (í•œêµ­ì–´ ì œëª© ì—†ìœ¼ë©´ Google ìŠ¤í¬ë˜í•‘)"""
        original_name = name

        # 1ì°¨: ì˜ì–´ë¡œ ë²ˆì—­ í›„ ê²€ìƒ‰
        translated_name = await translate_to_english(name)
        result = await ContentSearcher._search_manga_direct(session, translated_name)

        # í•œêµ­ì–´ ì œëª©ì´ ìˆìœ¼ë©´ ë°˜í™˜
        if result[0] is not None and await is_korean(result[0]):
            return result

        # 3ì°¨: Google ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ ì •ë³´ ì¶”ì¶œ
        print(f"ğŸ” MangaDexì—ì„œ í•œêµ­ì–´ ì œëª© ëª» ì°¾ìŒ, Google ìŠ¤í¬ë˜í•‘ ì‹œë„: {original_name}")
        google_info = await scrape_google_search_info(original_name, "ë§Œí™”")

        if google_info and google_info.get('title'):
            print(f"ğŸ” Googleì—ì„œ ì¶”ì¶œí•œ ì •ë³´ - ì œëª©: {google_info['title']}, ì‘ê°€: {google_info.get('author')}")
            # ì´ë¯¸ì§€ëŠ” ì›ë˜ MangaDex ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ None
            img_url = result[3] if result else None
            return google_info['title'], google_info.get('year'), google_info.get('author'), img_url

        # 4ì°¨: ê²€ìƒ‰ ê²°ê³¼ê°€ ì „í˜€ ì—†ìœ¼ë©´ ì›ë³¸ ì œëª© ë°˜í™˜
        if result[0] is not None:
            return result

        print(f"âŒ MangaDex/Google: No results for '{original_name}'")
        return None, None, None, None

    @staticmethod
    async def _search_naver_webtoon(session, name):
        """ë„¤ì´ë²„ ì›¹íˆ°ì—ì„œ ì§ì ‘ ê²€ìƒ‰ (ë‚´ë¶€ìš©)"""
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        try:
            search_url = f"https://comic.naver.com/api/search/all?keyword={name}"
            async with session.get(search_url, headers=headers) as response:
                if response.status == 200:
                    data = await response.json()
                    webtoons = data.get('searchWebtoonResult', {}).get('searchViewList', [])

                    if webtoons:
                        webtoon = webtoons[0]
                        title = webtoon.get('titleName', name)
                        author = webtoon.get('displayAuthor')
                        img_url = webtoon.get('thumbnailUrl')

                        return title, "ë„¤ì´ë²„ì›¹íˆ°", author, img_url
        except Exception as e:
            print(f"âš ï¸ Naver webtoon search failed: {e}")

        return None, None, None, None

    @staticmethod
    async def search_webtoon(session, name):
        """ì›¹íˆ° ê²€ìƒ‰ (ë„¤ì´ë²„ â†’ ì¹´ì¹´ì˜¤ â†’ Google ìŠ¤í¬ë˜í•‘)"""
        # 1ì°¨: ë„¤ì´ë²„ ì›¹íˆ° ê²€ìƒ‰
        result = await ContentSearcher._search_naver_webtoon(session, name)
        if result[0] is not None:
            return result

        # 3ì°¨: Google ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ ì›¹íˆ° ì •ë³´ ì¶”ì¶œ
        print(f"ğŸ” ì›¹íˆ° ì§ì ‘ ê²€ìƒ‰ ì‹¤íŒ¨, Google ìŠ¤í¬ë˜í•‘ ì‹œë„: {name}")
        google_info = await scrape_google_search_info(name, "ì›¹íˆ°")

        if google_info and google_info.get('title'):
            print(f"ğŸ” Googleì—ì„œ ì¶”ì¶œí•œ ì •ë³´ - ì œëª©: {google_info['title']}, ì‘ê°€: {google_info.get('author')}")
            return google_info['title'], "Google ê²€ìƒ‰", google_info.get('author'), None

        print(f"âŒ Webtoon: No results for '{name}'")
        return None, None, None, None
