import aiohttp
import asyncio
import os
import re
import requests
from bs4 import BeautifulSoup
from googletrans import Translator

TMDB_API_KEY = os.getenv("TMDB_API")
translator = Translator()


async def scrape_google_search_info(query, content_type="ë§Œí™”"):
    """ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì œëª©, ì‘ê°€ ì •ë³´ ì¶”ì¶œ (ë‚˜ë¬´ìœ„í‚¤ â†’ ìœ„í‚¤ë°±ê³¼ â†’ êµ¬ê¸€ ìˆœì„œ)"""
    try:
        await asyncio.sleep(1)  # rate limit ë°©ì§€
        loop = asyncio.get_event_loop()

        def _scrape():
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }

            # 1ì°¨ ì‹œë„: ë‚˜ë¬´ìœ„í‚¤ ì§ì ‘ ì ‘ê·¼ (ê°€ì¥ ë¹ ë¦„)
            print(f"  [1] ë‚˜ë¬´ìœ„í‚¤ì—ì„œ '{query}' ê²€ìƒ‰ ì¤‘...")
            namu_result = _scrape_namu_wiki(query, headers)
            if namu_result:
                return namu_result

            # 2ì°¨ ì‹œë„: ìœ„í‚¤ë°±ê³¼ API
            print(f"  [2] ìœ„í‚¤ë°±ê³¼ì—ì„œ '{query}' ê²€ìƒ‰ ì¤‘...")
            wiki_result = _scrape_wikipedia(query, headers)
            if wiki_result:
                return wiki_result

            # 3ì°¨ ì‹œë„: êµ¬ê¸€ ê²€ìƒ‰ (ëŠë¦¬ì§€ë§Œ ìµœí›„ì˜ ìˆ˜ë‹¨)
            print(f"  [3] êµ¬ê¸€ì—ì„œ '{query} {content_type}' ê²€ìƒ‰ ì¤‘...")
            google_result = _scrape_google_search(query, content_type, headers)
            if google_result:
                return google_result

            return None

        result = await loop.run_in_executor(None, _scrape)
        return result

    except Exception as e:
        print(f"âŒ ìŠ¤í¬ë˜í•‘ ì—ëŸ¬: {e}")
        return None


def _scrape_namu_wiki(query, headers):
    """ë‚˜ë¬´ìœ„í‚¤ì—ì„œ ì •ë³´ ì¶”ì¶œ"""
    try:
        url = f"https://namu.wiki/w/{query}"
        response = requests.get(url, headers=headers, timeout=5)

        if response.status_code == 404:
            return None

        if response.status_code != 200:
            return None

        soup = BeautifulSoup(response.text, 'html.parser')

        # ì œëª© ì¶”ì¶œ
        title_elem = soup.find('h1', {'class': 'wiki-title'})
        if not title_elem:
            return None

        title = title_elem.get_text(strip=True)

        # ì‘ê°€/ì €ì ì •ë³´ ì¶”ì¶œ (ì •ë³´ ë°•ìŠ¤ì—ì„œ)
        author = None
        year = None

        # ì •ë³´ ë°•ìŠ¤ ì°¾ê¸°
        info_box = soup.find('table', {'class': 'wikitable'})
        if info_box:
            rows = info_box.find_all('tr')
            for row in rows:
                cells = row.find_all('td')
                if len(cells) >= 2:
                    label = cells[0].get_text(strip=True).lower()
                    value = cells[1].get_text(strip=True)

                    # ì‘ê°€/ì €ì/ì›ì‘ ì°¾ê¸°
                    if any(k in label for k in ['ì‘ê°€', 'ì €ì', 'ì›ì‘', 'ì‘í™”', 'ê°ë³¸']):
                        if not author:
                            author = value
                            break

        # ì—°ë„ ì •ê·œì‹ìœ¼ë¡œ ì¶”ì¶œ
        year_match = re.search(r'(20\d{2})', response.text)
        if year_match:
            year = year_match.group(1)

        return {
            'title': title,
            'author': author or "ì •ë³´ ì—†ìŒ",
            'year': year,
            'img_url': None,
            'source': 'ë‚˜ë¬´ìœ„í‚¤'
        }

    except Exception as e:
        print(f"    âš ï¸ ë‚˜ë¬´ìœ„í‚¤ íŒŒì‹± ì‹¤íŒ¨: {e}")
        return None


def _scrape_wikipedia(query, headers):
    try:
        api = "https://ko.wikipedia.org/w/api.php"

        # 1) ê²€ìƒ‰í•´ì„œ title í™•ë³´
        search_params = {
            'action': 'query',
            'list': 'search',
            'srsearch': query,
            'format': 'json',
            'srlimit': 1
        }
        r = requests.get(api, params=search_params, headers=headers, timeout=5)
        data = r.json()
        results = data.get('query', {}).get('search', [])
        if not results:
            return None

        title = results[0]['title']

        # 2) titleë¡œ ë³¸ë¬¸ ìš”ì•½ + ì¸ë„¤ì¼ ê°€ì ¸ì˜¤ê¸°
        page_params = {
            'action': 'query',
            'format': 'json',
            'prop': 'extracts|pageimages',
            'titles': title,
            'exintro': 1,
            'explaintext': 1,
            'piprop': 'thumbnail',
            'pithumbsize': 400
        }
        r2 = requests.get(api, params=page_params, headers=headers, timeout=5)
        data2 = r2.json()

        pages = data2.get('query', {}).get('pages', {})
        page = next(iter(pages.values()), None)
        if not page:
            return None

        extract = page.get('extract', '') or ''
        thumb = (page.get('thumbnail') or {}).get('source')

        # 3) ìš”ì•½ë¬¸ì—ì„œ ì‘ê°€/ì—°ë„ ì¶”ì¶œ(íœ´ë¦¬ìŠ¤í‹±)
        author = _extract_author_from_kowiki_extract(extract)
        year = _extract_year_from_text(extract)

        return {
            'title': title,
            'author': author or "ì •ë³´ ì—†ìŒ",
            'year': year,              # ë¬¸ìì—´ "2022" ê°™ì€ í˜•íƒœ
            'img_url': thumb,
            'source': 'ìœ„í‚¤ë°±ê³¼'
        }

    except Exception as e:
        print(f"    âš ï¸ ìœ„í‚¤ë°±ê³¼ íŒŒì‹± ì‹¤íŒ¨: {e}")
        return None


def _extract_author_from_kowiki_extract(text: str):
    """
    í•œêµ­ì–´ ìœ„í‚¤ ìš”ì•½ë¬¸ì—ì„œ í”íˆ ë‚˜ì˜¤ëŠ” íŒ¨í„´:
    - '...ëŠ” OOOê°€ ...' / '...ëŠ” OOOì˜ ...' / '...ëŠ” OOOì´ ...'
    """
    t = " ".join(text.split())
    patterns = [
        r'([ê°€-í£A-Za-zÂ·\s]+?)ê°€\s+(?:ì“°ê³ \s+ê·¸ë¦°|ê·¸ë¦°|ì“´)\s+(?:ì¼ë³¸\s+)?(?:ë§Œí™”|ì†Œì„¤|ì‘í’ˆ)',
        r'([ê°€-í£A-Za-zÂ·\s]+?)ì˜\s+(?:ì¼ë³¸\s+)?(?:ë§Œí™”|ì†Œì„¤|ì‘í’ˆ)',
        r'ì›ì‘[:\s]*([ê°€-í£A-Za-zÂ·\s]+)',
        r'ì‘ê°€[:\s]*([ê°€-í£A-Za-zÂ·\s]+)',
        r'ê¸€[:\s]*([ê°€-í£A-Za-zÂ·\s]+)',
        r'ê·¸ë¦¼[:\s]*([ê°€-í£A-Za-zÂ·\s]+)',
    ]
    for p in patterns:
        m = re.search(p, t)
        if m:
            return m.group(1).strip()
    return None


def _extract_year_from_text(text: str):
    # ê°€ì¥ ë¨¼ì € ë“±ì¥í•˜ëŠ” 4ìë¦¬ ì—°ë„(2000~2099) ì¶”ì¶œ
    m = re.search(r'\b(20\d{2})\b', text)
    return m.group(1) if m else None

def _scrape_google_search(query, content_type, headers):
    """êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì •ë³´ ì¶”ì¶œ (ìµœí›„ì˜ ìˆ˜ë‹¨)"""
    try:
        search_url = f"https://www.google.com/search?q={query}+{content_type}&hl=ko"
        response = requests.get(search_url, headers=headers, timeout=5)

        if response.status_code != 200:
            return None

        soup = BeautifulSoup(response.text, 'html.parser')

        # ê²€ìƒ‰ ê²°ê³¼ snippetì—ì„œ ì²« ë²ˆì§¸ ê²°ê³¼ ì°¾ê¸°
        search_results = soup.find_all('div', {'class': 'g'})

        for result in search_results:
            # ì œëª© ì°¾ê¸°
            title_elem = result.find('h3')
            if not title_elem:
                continue

            title = title_elem.get_text(strip=True)

            # snippetì—ì„œ ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
            snippet_elem = result.find('div', {'style': '-webkit-line-clamp:2'})
            snippet = snippet_elem.get_text(strip=True) if snippet_elem else ""

            # ì‘ê°€ ì •ë³´ ì¶”ì¶œ ì‹œë„
            author = None
            if 'ì‘ê°€' in snippet or 'ì €ì' in snippet:
                parts = snippet.split('ì‘ê°€')[-1] if 'ì‘ê°€' in snippet else snippet.split('ì €ì')[-1]
                author = parts.split(',')[0].strip()[:50]

            if title:  # ì œëª©ì´ ìˆìœ¼ë©´ ë°˜í™˜
                return {
                    'title': title,
                    'author': author or "ì •ë³´ ì—†ìŒ",
                    'year': None,
                    'img_url': None,
                    'source': 'êµ¬ê¸€ ê²€ìƒ‰'
                }

        return None

    except Exception as e:
        print(f"    âš ï¸ êµ¬ê¸€ ê²€ìƒ‰ íŒŒì‹± ì‹¤íŒ¨: {e}")
        return None


async def is_korean(text):
    """í•œê¸€ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
    if not text:
        return False
    return bool(re.search('[ê°€-í£]', text))


async def translate_to_korean(text):
    """ì˜ì–´/ì¼ë³¸ì–´ ì´ë¦„ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­"""
    if not text or text == "N/A" or await is_korean(text):
        return text

    try:
        result = await translator.translate(text, dest='ko')
        return result.text
    except Exception as e:
        print(f"Google Translate failed: {e}")
        # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
        return text
    
async def translate_to_english(text):
    if not text or text == "N/A":
        return text

    try:
        result = await translator.translate(text, dest='en')
        return result.text
    except Exception as e:
        print(f"Google Translate failed: {e}")
        # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
        return text

class ContentSearcher:
    """ì˜í™”, ë§Œí™”, ì›¹íˆ° ê²€ìƒ‰ í†µí•© í´ë˜ìŠ¤"""

    @staticmethod
    async def _search_tmdb_direct(session, name):
        """TMDBì—ì„œ ì§ì ‘ ê²€ìƒ‰ (ë‚´ë¶€ìš©)"""
        search_url = f"https://api.themoviedb.org/3/search/multi?api_key={TMDB_API_KEY}&query={name}&language=ko-KR"
        async with session.get(search_url) as response:
            data = await response.json()

        if data.get('results'):
            results = [r for r in data['results'] if r.get('media_type') in ('movie', 'tv')]
            if not results:
                return name, "N/A", "N/A", None, "movie"

            item = results[0]
            media_type = item.get('media_type')
            genre_ids = item.get('genre_ids', [])

            is_animation = 16 in genre_ids
            if is_animation:
                category = 'anime'
            elif media_type == 'tv':
                category = 'drama'
            else:
                category = 'movie'

            if media_type == 'movie':
                title = item.get('title', name)
                year = item['release_date'][:4] if item.get('release_date') else "N/A"
            else:
                title = item.get('name', name)
                year = item['first_air_date'][:4] if item.get('first_air_date') else "N/A"

            item_id = item['id']
            director = None

            if not await is_korean(title):
                title = await translate_to_korean(title)

            if media_type == 'movie':
                credits_url = f"https://api.themoviedb.org/3/movie/{item_id}/credits?api_key={TMDB_API_KEY}&language=ko-KR"
                async with session.get(credits_url) as credits_response:
                    credits = await credits_response.json()
                director_info = next((crew for crew in credits.get('crew', []) if crew['job'] == 'Director'), None)

                if director_info:
                    director = director_info.get('name')
                    if not await is_korean(director):
                        director = await translate_to_korean(director)
            else:
                details_url = f"https://api.themoviedb.org/3/tv/{item_id}?api_key={TMDB_API_KEY}&language=ko-KR"
                async with session.get(details_url) as details_response:
                    details = await details_response.json()
                creators = details.get('created_by', [])
                if creators:
                    director = creators[0]['name']
                    if not await is_korean(director):
                        director = await translate_to_korean(director)

            poster_path = item.get('poster_path')
            img_url = f"https://image.tmdb.org/t/p/w500{poster_path}" if poster_path else None

            return title, year, director, img_url, category

        return None, None, None, None, None

    @staticmethod
    async def search_tmdb(session, name):
        """TMDB multi search APIë¡œ ì˜í™”/ë“œë¼ë§ˆ/ì• ë‹ˆ ê²€ìƒ‰ ë° ìë™ ë¶„ë¥˜ (Google fallback í¬í•¨)"""
        # 1ì°¨: ì§ì ‘ ê²€ìƒ‰
        result = await ContentSearcher._search_tmdb_direct(session, name)

        # ê²€ìƒ‰ ì„±ê³µ ì‹œ ë°˜í™˜
        if result[0] is not None:
            return result

        # 2ì°¨: Google ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ ì •ë³´ ì¶”ì¶œ
        print(f"ğŸ” TMDB ì§ì ‘ ê²€ìƒ‰ ì‹¤íŒ¨, Google ìŠ¤í¬ë˜í•‘ ì‹œë„: {name}")
        google_info = await scrape_google_search_info(name, "ì˜í™”")

        if google_info and google_info.get('title'):
            print(f"ğŸ” Googleì—ì„œ ì¶”ì¶œí•œ ì •ë³´ - ì œëª©: {google_info['title']}, ê°ë…: {google_info.get('author')}")
            return google_info['title'], google_info.get('year'), google_info.get('author'), None, 'movie'

        # 3ì°¨: ë²ˆì—­ í›„ ì¬ê²€ìƒ‰
        translated = await translate_to_english(name)
        if translated and translated != name:
            print(f"ğŸ” ë²ˆì—­ëœ ì œëª©ìœ¼ë¡œ TMDB ì¬ê²€ìƒ‰: {translated}")
            result = await ContentSearcher._search_tmdb_direct(session, translated)
            if result[0] is not None:
                return result

        return None, None, None, None, None

    @staticmethod
    async def _search_manga_direct(session, name):
        """MangaDexì—ì„œ ì§ì ‘ ê²€ìƒ‰ (ë‚´ë¶€ìš©)"""
        url = f"https://api.mangadex.org/manga?title={name}&limit=1&includes[]=author&includes[]=cover_art"

        try:
            async with session.get(url) as response:
                data = await response.json()

            if data.get('data') and len(data['data']) > 0:
                manga = data['data'][0]
                attributes = manga.get('attributes', {})
                title_dict = attributes.get('title', {})
                alt_titles = attributes.get('altTitles', [])

                title = None
                # 1. title ê°ì²´ì—ì„œ í•œêµ­ì–´ ì œëª© ì°¾ê¸°
                if 'ko' in title_dict:
                    title = title_dict['ko']

                # 2. altTitlesì—ì„œ í•œêµ­ì–´ ì œëª© ì°¾ê¸°
                if not title:
                    for alt in alt_titles:
                        if 'ko' in alt:
                            title = alt['ko']
                            break

                # 3. title ê°ì²´ì—ì„œ ì˜ì–´ ì œëª© ì°¾ê¸°
                if not title and 'en' in title_dict:
                    title = title_dict['en']

                # 4. ê·¸ë˜ë„ ì—†ìœ¼ë©´ ì²«ë²ˆì§¸ ì œëª© ì‚¬ìš©
                if not title:
                    title = list(title_dict.values())[0] if title_dict else name

                year = str(attributes.get('year')) if attributes.get('year') else None
                author = None
                relationships = manga.get('relationships', [])
                for rel in relationships:
                    if rel.get('type') == 'author':
                        author_attrs = rel.get('attributes', {})
                        author = author_attrs.get('name')
                        if author and not await is_korean(author):
                            author = await translate_to_korean(author)
                        break

                img_url = None
                for rel in relationships:
                    if rel.get('type') == 'cover_art':
                        cover_attrs = rel.get('attributes', {})
                        filename = cover_attrs.get('fileName')
                        if filename:
                            manga_id = manga.get('id')
                            img_url = f"https://uploads.mangadex.org/covers/{manga_id}/{filename}"
                        break

                return title, year, author, img_url

        except Exception as e:
            print(f"âŒ MangaDex API error: {e}")

        return None, None, None, None

    @staticmethod
    async def search_manga(session, name):
        """MangaDexì—ì„œ ë§Œí™” ê²€ìƒ‰ (í•œêµ­ì–´ ì œëª© ì—†ìœ¼ë©´ Google ìŠ¤í¬ë˜í•‘)"""
        original_name = name

        # 1ì°¨: ì˜ì–´ë¡œ ë²ˆì—­ í›„ ê²€ìƒ‰
        translated_name = await translate_to_english(name)
        result = await ContentSearcher._search_manga_direct(session, translated_name)

        # í•œêµ­ì–´ ì œëª©ì´ ìˆìœ¼ë©´ ë°˜í™˜
        if result[0] is not None and await is_korean(result[0]):
            return result

        # 3ì°¨: Google ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ ì •ë³´ ì¶”ì¶œ
        print(f"ğŸ” MangaDexì—ì„œ í•œêµ­ì–´ ì œëª© ëª» ì°¾ìŒ, Google ìŠ¤í¬ë˜í•‘ ì‹œë„: {original_name}")
        google_info = await scrape_google_search_info(original_name, "ë§Œí™”")

        if google_info and google_info.get('title'):
            print(f"ğŸ” Googleì—ì„œ ì¶”ì¶œí•œ ì •ë³´ - ì œëª©: {google_info['title']}, ì‘ê°€: {google_info.get('author')}")
            # ì´ë¯¸ì§€ëŠ” ì›ë˜ MangaDex ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ None
            img_url = result[3] if result else None
            return google_info['title'], google_info.get('year'), google_info.get('author'), img_url

        print(f"âŒ MangaDex/Google: No results for '{original_name}'")
        return None, None, None, None

    @staticmethod
    async def _search_naver_webtoon(session, name):
        """ë„¤ì´ë²„ ì›¹íˆ°ì—ì„œ ì§ì ‘ ê²€ìƒ‰ (ë‚´ë¶€ìš©)"""
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        try:
            search_url = f"https://comic.naver.com/api/search/all?keyword={name}"
            async with session.get(search_url, headers=headers) as response:
                if response.status == 200:
                    data = await response.json()
                    webtoons = data.get('searchWebtoonResult', {}).get('searchViewList', [])

                    if webtoons:
                        webtoon = webtoons[0]
                        title = webtoon.get('titleName', name)
                        author = webtoon.get('displayAuthor')
                        img_url = webtoon.get('thumbnailUrl')

                        return title, "ë„¤ì´ë²„ì›¹íˆ°", author, img_url
        except Exception as e:
            print(f"âš ï¸ Naver webtoon search failed: {e}")

        return None, None, None, None

    @staticmethod
    async def search_webtoon(session, name):
        """ì›¹íˆ° ê²€ìƒ‰ (ë„¤ì´ë²„ â†’ ì¹´ì¹´ì˜¤ â†’ Google ìŠ¤í¬ë˜í•‘)"""
        # 1ì°¨: ë„¤ì´ë²„ ì›¹íˆ° ê²€ìƒ‰
        result = await ContentSearcher._search_naver_webtoon(session, name)
        if result[0] is not None:
            return result

        # 3ì°¨: Google ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ ì›¹íˆ° ì •ë³´ ì¶”ì¶œ
        print(f"ğŸ” ì›¹íˆ° ì§ì ‘ ê²€ìƒ‰ ì‹¤íŒ¨, Google ìŠ¤í¬ë˜í•‘ ì‹œë„: {name}")
        google_info = await scrape_google_search_info(name, "ì›¹íˆ°")

        if google_info and google_info.get('title'):
            print(f"ğŸ” Googleì—ì„œ ì¶”ì¶œí•œ ì •ë³´ - ì œëª©: {google_info['title']}, ì‘ê°€: {google_info.get('author')}")
            return google_info['title'], "Google ê²€ìƒ‰", google_info.get('author'), None

        print(f"âŒ Webtoon: No results for '{name}'")
        return None, None, None, None
