import aiohttp
import asyncio
import os
import re
import requests
from bs4 import BeautifulSoup
from googletrans import Translator

TMDB_API_KEY = os.getenv("TMDB_API")
translator = Translator()


async def scrape_google_search_info(query, content_type="ë§Œí™”"):
    """ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì œëª©, ì‘ê°€ ì •ë³´ ì¶”ì¶œ (ë‚˜ë¬´ìœ„í‚¤ â†’ ìœ„í‚¤ë°±ê³¼ â†’ êµ¬ê¸€ ìˆœì„œ)"""
    try:
        await asyncio.sleep(1)  # rate limit ë°©ì§€
        loop = asyncio.get_event_loop()

        def _scrape():
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }

            # 1ì°¨ ì‹œë„: ë‚˜ë¬´ìœ„í‚¤ ì§ì ‘ ì ‘ê·¼ (ê°€ì¥ ë¹ ë¦„)
            print(f"  [1] ë‚˜ë¬´ìœ„í‚¤ì—ì„œ '{query}' ê²€ìƒ‰ ì¤‘...")
            namu_result = _scrape_namu_wiki(query, headers)
            if namu_result:
                return namu_result

            # 2ì°¨ ì‹œë„: ìœ„í‚¤ë°±ê³¼ API
            print(f"  [2] ìœ„í‚¤ë°±ê³¼ì—ì„œ '{query}' ê²€ìƒ‰ ì¤‘...")
            wiki_result = _scrape_wikipedia(query, headers)
            if wiki_result:
                return wiki_result

            # 3ì°¨ ì‹œë„: êµ¬ê¸€ ê²€ìƒ‰ (ëŠë¦¬ì§€ë§Œ ìµœí›„ì˜ ìˆ˜ë‹¨)
            print(f"  [3] êµ¬ê¸€ì—ì„œ '{query} {content_type}' ê²€ìƒ‰ ì¤‘...")
            google_result = _scrape_google_search(query, content_type, headers)
            if google_result:
                return google_result

            return None

        result = await loop.run_in_executor(None, _scrape)
        return result

    except Exception as e:
        print(f"âŒ ìŠ¤í¬ë˜í•‘ ì—ëŸ¬: {e}")
        return None


def _scrape_namu_wiki(query, headers):
    """ë‚˜ë¬´ìœ„í‚¤ì—ì„œ ì •ë³´ ì¶”ì¶œ"""
    try:
        url = f"https://namu.wiki/w/{query}"
        response = requests.get(url, headers=headers, timeout=5)

        if response.status_code == 404:
            return None

        if response.status_code != 200:
            return None

        soup = BeautifulSoup(response.text, 'html.parser')

        # ì œëª© ì¶”ì¶œ
        title_elem = soup.find('h1', {'class': 'wiki-title'})
        if not title_elem:
            return None

        title = title_elem.get_text(strip=True)

        # ì‘ê°€/ì €ì ì •ë³´ ì¶”ì¶œ (ì •ë³´ ë°•ìŠ¤ì—ì„œ)
        author = None
        year = None

        # ì •ë³´ ë°•ìŠ¤ ì°¾ê¸°
        info_box = soup.find('table', {'class': 'wikitable'})
        if info_box:
            rows = info_box.find_all('tr')
            for row in rows:
                cells = row.find_all('td')
                if len(cells) >= 2:
                    label = cells[0].get_text(strip=True).lower()
                    value = cells[1].get_text(strip=True)

                    # ì‘ê°€/ì €ì/ì›ì‘ ì°¾ê¸°
                    if any(k in label for k in ['ì‘ê°€', 'ì €ì', 'ì›ì‘', 'ì‘í™”', 'ê°ë³¸']):
                        if not author:
                            author = value
                            break

        # ì—°ë„ ì •ê·œì‹ìœ¼ë¡œ ì¶”ì¶œ
        year_match = re.search(r'(20\d{2})', response.text)
        if year_match:
            year = year_match.group(1)

        return {
            'title': title,
            'author': author or "ì •ë³´ ì—†ìŒ",
            'year': year,
            'img_url': None,
            'source': 'ë‚˜ë¬´ìœ„í‚¤'
        }

    except Exception as e:
        print(f"    âš ï¸ ë‚˜ë¬´ìœ„í‚¤ íŒŒì‹± ì‹¤íŒ¨: {e}")
        return None


def _clean_wikitext_minimal(wt: str) -> str:
    """
    ìœ„í‚¤í…ìŠ¤íŠ¸ì—ì„œ ì‘ê°€/ì—°ë„ ì¶”ì¶œì„ ìœ„í•´ ìµœì†Œí•œì˜ ë…¸ì´ì¦ˆë§Œ ì œê±°.
    (ì™„ë²½í•œ íŒŒì„œê°€ ì•„ë‹ˆë¼ 'ì¶”ì¶œìš©'ìœ¼ë¡œë§Œ)
    """
    if not wt:
        return ""

    t = wt

    # ì£¼ì„ ì œê±°
    t = re.sub(r"<!--.*?-->", " ", t, flags=re.DOTALL)

    # ref íƒœê·¸ ì œê±°
    t = re.sub(r"<ref[^>]*>.*?</ref>", " ", t, flags=re.DOTALL)
    t = re.sub(r"<ref[^/>]*/\s*>", " ", t)

    # í…œí”Œë¦¿/í…Œì´ë¸” ë“± ë„ˆë¬´ ë³µì¡í•œ ê²ƒ ì¼ë¶€ ì™„í™”(ê³¼ë„ ì œê±°ëŠ” ì—­íš¨ê³¼ë¼ ìµœì†Œ)
    # ë§í¬ [[A|B]] -> B, [[A]] -> A
    t = re.sub(r"\[\[([^|\]]+)\|([^\]]+)\]\]", r"\2", t)
    t = re.sub(r"\[\[([^\]]+)\]\]", r"\1", t)

    # ì™¸ë¶€ë§í¬ [http://... ë¼ë²¨] -> ë¼ë²¨
    t = re.sub(r"\[https?://[^\s\]]+\s+([^\]]+)\]", r"\1", t)

    # êµµê²Œ/ê¸°ìš¸ì„ ë§ˆí¬ì—… ì œê±°
    t = t.replace("'''", "").replace("''", "")

    # ë‚¨ì€ íƒœê·¸ ì œê±°
    t = re.sub(r"<[^>]+>", " ", t)

    # ê³µë°± ì •ë¦¬
    t = re.sub(r"\s+", " ", t).strip()
    return t


def _extract_field_from_infobox(wikitext: str, keys):
    """
    ì¸í¬ë°•ìŠ¤/í…œí”Œë¦¿ì—ì„œ ' | í‚¤ = ê°’ ' í˜•íƒœë¥¼ ìš°ì„  ì¶”ì¶œ.
    keys: ['ì‘ê°€', 'ì €ì', ...]
    """
    if not wikitext:
        return None

    # ì¸í¬ë°•ìŠ¤ê°€ ëª…ì‹œëœ ê²½ìš°ê°€ ë§ì§€ë§Œ í…œí”Œë¦¿ ì´ë¦„ì´ ë‹¤ì–‘í•´ì„œ 'í‚¤=' ë¼ì¸ì„ ì§ì ‘ ì°¾ëŠ” ë°©ì‹
    # ì˜ˆ: | ì‘ê°€ = ìš°ë¼ë‚˜ ì¼€ì´
    #     | ì €ì = ...
    for k in keys:
        # ì¤„ ë‹¨ìœ„ì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ì°¾ê¸° ìœ„í•´ line anchor ì‚¬ìš©
        m = re.search(rf"(?m)^\s*\|\s*{re.escape(k)}\s*=\s*(.+?)\s*$", wikitext)
        if m:
            value = m.group(1).strip()
            # ê°’ ëì— ì£¼ì„/í…œí”Œë¦¿/ref ë“±ì´ ë¶™ì„ ìˆ˜ ìˆìœ¼ë‹ˆ ê°„ë‹¨íˆ ì»·
            value = re.split(r"<ref|{{|}}|\[\[ë¶„ë¥˜:|\[\[Category:", value)[0].strip()
            # ë§í¬ [[A|B]] ê°™ì€ ê²½ìš°ëŠ” ìµœì†Œ ì¹˜í™˜
            value = re.sub(r"\[\[([^|\]]+)\|([^\]]+)\]\]", r"\2", value)
            value = re.sub(r"\[\[([^\]]+)\]\]", r"\1", value)
            return value[:80]
    return None


def _extract_year_from_text(text: str):
    if not text:
        return None
    m = re.search(r"(20\d{2})\s*ë…„?", text)
    return m.group(1) if m else None


def _extract_author_from_text(text: str):
    """
    ì¸í¬ë°•ìŠ¤ì—ì„œ ëª» ì°¾ì•˜ì„ ë•Œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ì—ì„œ íœ´ë¦¬ìŠ¤í‹± ì¶”ì¶œ
    """
    if not text:
        return None

    t = " ".join(text.split())

    patterns = [
        r'([ê°€-í£A-Za-zÂ·\s]+?)ê°€\s+(?:ì“°ê³ \s+ê·¸ë¦°|ê·¸ë¦°|ì“´)\s+(?:ì¼ë³¸\s+)?(?:ë§Œí™”|ì†Œì„¤|ì‘í’ˆ)',
        r'([ê°€-í£A-Za-zÂ·\s]+?)ì˜\s+(?:ì¼ë³¸\s+)?(?:ë§Œí™”|ì†Œì„¤|ì‘í’ˆ)',
        r'(?:ì‘ê°€|ì €ì|ì›ì‘|ê¸€|ê°ë³¸|ì‘í™”|ê°ë…)\s*[:ï¼š]\s*([ê°€-í£A-Za-zÂ·\s]+)',
        r'(?:ì‘ê°€|ì €ì|ì›ì‘|ê¸€|ê°ë³¸|ì‘í™”|ê°ë…)\s+([ê°€-í£A-Za-zÂ·\s]+)',
    ]
    for p in patterns:
        m = re.search(p, t)
        if m:
            return m.group(1).strip()[:60]
    return None


def _scrape_wikipedia(query: str, headers: dict):
    """
    ìœ„í‚¤ë°±ê³¼(ko)ì—ì„œ:
      1) searchë¡œ pageid/title í™•ë³´
      2) pageidë¡œ revisions(content) = í†µì§œ ìœ„í‚¤í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° (extracts ë¯¸ì‚¬ìš©)
      3) pageimagesë¡œ ì¸ë„¤ì¼ ê°€ì ¸ì˜¤ê¸°
      4) ìœ„í‚¤í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ì‘ê°€/ì—°ë„ ì¶”ì¶œ
    """
    try:
        api = "https://ko.wikipedia.org/w/api.php"

        # 1) ê²€ìƒ‰ -> pageid/title
        search_params = {
            "action": "query",
            "list": "search",
            "srsearch": query,
            "format": "json",
            "srlimit": 1,
        }
        r = requests.get(api, params=search_params, headers=headers, timeout=5)
        r.raise_for_status()
        data = r.json()

        results = data.get("query", {}).get("search", [])
        if not results:
            return None

        title = results[0].get("title")
        pageid = results[0].get("pageid")
        if not title or not pageid:
            return None

        # 2) í†µì§œ ìœ„í‚¤í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
        content_params = {
            "action": "query",
            "format": "json",
            "pageids": pageid,
            "prop": "revisions",
            "rvslots": "main",
            "rvprop": "content",
            "redirects": 1,
        }
        r2 = requests.get(api, params=content_params, headers=headers, timeout=5)
        r2.raise_for_status()
        data2 = r2.json()

        pages = data2.get("query", {}).get("pages", {})
        page = pages.get(str(pageid)) or next(iter(pages.values()), None)
        if not page or "missing" in page:
            return None

        revs = page.get("revisions", [])
        if not revs:
            return None

        # MediaWiki slots êµ¬ì¡°
        wikitext = revs[0].get("slots", {}).get("main", {}).get("*")
        if not wikitext:
            # êµ¬í˜• êµ¬ì¡° fallback
            wikitext = revs[0].get("*")
        if not wikitext:
            return None

        # 3) ì¸ë„¤ì¼ ë³„ë„ ìš”ì²­(pageimages)
        img_params = {
            "action": "query",
            "format": "json",
            "pageids": pageid,
            "prop": "pageimages",
            "piprop": "thumbnail",
            "pithumbsize": 400,
            "redirects": 1,
        }
        r3 = requests.get(api, params=img_params, headers=headers, timeout=5)
        r3.raise_for_status()
        data3 = r3.json()
        pages3 = data3.get("query", {}).get("pages", {})
        page3 = pages3.get(str(pageid)) or next(iter(pages3.values()), None)
        img_url = (page3.get("thumbnail") or {}).get("source") if page3 else None

        # 4) ì‘ê°€/ì—°ë„ ì¶”ì¶œ
        # 4-1) ì¸í¬ë°•ìŠ¤/í…œí”Œë¦¿ì—ì„œ í‚¤ ê¸°ë°˜ ìš°ì„  ì¶”ì¶œ
        author = _extract_field_from_infobox(
            wikitext,
            keys=["ì‘ê°€", "ì €ì", "ì›ì‘", "ê¸€", "ê°ë³¸", "ì‘í™”", "ê°ë…"]
        )

        # 4-2) ì—°ë„ë„ ì¸í¬ë°•ìŠ¤ í‚¤ ìš°ì„  (ìˆìœ¼ë©´)
        year = _extract_field_from_infobox(
            wikitext,
            keys=["ì—°ë„", "ì¶œì‹œ", "ë°œë§¤", "ê°œë´‰", "ì—°ì¬ ì‹œì‘", "ì—°ì¬ì‹œì‘", "ì²« ì—°ì¬", "ì²«ë°©ì†¡", "ë°©ì˜ ì‹œì‘"]
        )
        # yearê°€ "2022ë…„ 2ì›” 16ì¼" ê°™ì€ í˜•íƒœë©´ 4ìë¦¬ë§Œ ë½‘ê¸°
        year = _extract_year_from_text(year) if year else None

        # 4-3) ì¸í¬ë°•ìŠ¤ì—ì„œ ëª» ì°¾ì•˜ìœ¼ë©´ ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¡œ fallback
        cleaned = _clean_wikitext_minimal(wikitext)
        if not author:
            author = _extract_author_from_text(cleaned)
        if not year:
            year = _extract_year_from_text(cleaned)

        return {
            "title": title,
            "pageid": pageid,
            "author": author or "ì •ë³´ ì—†ìŒ",
            "year": year,
            "img_url": img_url,
        }

    except requests.RequestException as e:
        print(f"    âš ï¸ ìœ„í‚¤ë°±ê³¼ ìš”ì²­ ì‹¤íŒ¨: {e}")
        return None
    except Exception as e:
        print(f"    âš ï¸ ìœ„í‚¤ë°±ê³¼ íŒŒì‹± ì‹¤íŒ¨: {e}")
        return None

def _scrape_google_search(query, content_type, headers):
    """êµ¬ê¸€ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ì •ë³´ ì¶”ì¶œ (ìµœí›„ì˜ ìˆ˜ë‹¨)"""
    try:
        search_url = f"https://www.google.com/search?q={query}+{content_type}&hl=ko"
        response = requests.get(search_url, headers=headers, timeout=5)

        if response.status_code != 200:
            return None

        soup = BeautifulSoup(response.text, 'html.parser')

        # ê²€ìƒ‰ ê²°ê³¼ snippetì—ì„œ ì²« ë²ˆì§¸ ê²°ê³¼ ì°¾ê¸°
        search_results = soup.find_all('div', {'class': 'g'})

        for result in search_results:
            # ì œëª© ì°¾ê¸°
            title_elem = result.find('h3')
            if not title_elem:
                continue

            title = title_elem.get_text(strip=True)

            # snippetì—ì„œ ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
            snippet_elem = result.find('div', {'style': '-webkit-line-clamp:2'})
            snippet = snippet_elem.get_text(strip=True) if snippet_elem else ""

            # ì‘ê°€ ì •ë³´ ì¶”ì¶œ ì‹œë„
            author = None
            if 'ì‘ê°€' in snippet or 'ì €ì' in snippet:
                parts = snippet.split('ì‘ê°€')[-1] if 'ì‘ê°€' in snippet else snippet.split('ì €ì')[-1]
                author = parts.split(',')[0].strip()[:50]

            if title:  # ì œëª©ì´ ìˆìœ¼ë©´ ë°˜í™˜
                return {
                    'title': title,
                    'author': author or "ì •ë³´ ì—†ìŒ",
                    'year': None,
                    'img_url': None,
                    'source': 'êµ¬ê¸€ ê²€ìƒ‰'
                }

        return None

    except Exception as e:
        print(f"    âš ï¸ êµ¬ê¸€ ê²€ìƒ‰ íŒŒì‹± ì‹¤íŒ¨: {e}")
        return None


async def is_korean(text):
    """í•œê¸€ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
    if not text:
        return False
    return bool(re.search('[ê°€-í£]', text))


async def translate_to_korean(text):
    """ì˜ì–´/ì¼ë³¸ì–´ ì´ë¦„ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­"""
    if not text or text == "N/A" or await is_korean(text):
        return text

    try:
        result = await translator.translate(text, dest='ko')
        return result.text
    except Exception as e:
        print(f"Google Translate failed: {e}")
        # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
        return text
    
async def translate_to_english(text):
    if not text or text == "N/A":
        return text

    try:
        result = await translator.translate(text, dest='en')
        return result.text
    except Exception as e:
        print(f"Google Translate failed: {e}")
        # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
        return text

class ContentSearcher:
    """ì˜í™”, ë§Œí™”, ì›¹íˆ° ê²€ìƒ‰ í†µí•© í´ë˜ìŠ¤"""

    @staticmethod
    async def _search_tmdb_direct(session, name):
        """TMDBì—ì„œ ì§ì ‘ ê²€ìƒ‰ (ë‚´ë¶€ìš©)"""
        search_url = f"https://api.themoviedb.org/3/search/multi?api_key={TMDB_API_KEY}&query={name}&language=ko-KR"
        async with session.get(search_url) as response:
            data = await response.json()

        if data.get('results'):
            results = [r for r in data['results'] if r.get('media_type') in ('movie', 'tv')]
            if not results:
                return name, "N/A", "N/A", None, "movie"

            item = results[0]
            media_type = item.get('media_type')
            genre_ids = item.get('genre_ids', [])

            is_animation = 16 in genre_ids
            if is_animation:
                category = 'anime'
            elif media_type == 'tv':
                category = 'drama'
            else:
                category = 'movie'

            if media_type == 'movie':
                title = item.get('title', name)
                year = item['release_date'][:4] if item.get('release_date') else "N/A"
            else:
                title = item.get('name', name)
                year = item['first_air_date'][:4] if item.get('first_air_date') else "N/A"

            item_id = item['id']
            director = None

            if not await is_korean(title):
                title = await translate_to_korean(title)

            if media_type == 'movie':
                credits_url = f"https://api.themoviedb.org/3/movie/{item_id}/credits?api_key={TMDB_API_KEY}&language=ko-KR"
                async with session.get(credits_url) as credits_response:
                    credits = await credits_response.json()
                director_info = next((crew for crew in credits.get('crew', []) if crew['job'] == 'Director'), None)

                if director_info:
                    director = director_info.get('name')
                    if not await is_korean(director):
                        director = await translate_to_korean(director)
            else:
                details_url = f"https://api.themoviedb.org/3/tv/{item_id}?api_key={TMDB_API_KEY}&language=ko-KR"
                async with session.get(details_url) as details_response:
                    details = await details_response.json()
                creators = details.get('created_by', [])
                if creators:
                    director = creators[0]['name']
                    if not await is_korean(director):
                        director = await translate_to_korean(director)

            poster_path = item.get('poster_path')
            img_url = f"https://image.tmdb.org/t/p/w500{poster_path}" if poster_path else None

            return title, year, director, img_url, category

        return None, None, None, None, None

    @staticmethod
    async def search_tmdb(session, name):
        """TMDB multi search APIë¡œ ì˜í™”/ë“œë¼ë§ˆ/ì• ë‹ˆ ê²€ìƒ‰ ë° ìë™ ë¶„ë¥˜ (Google fallback í¬í•¨)"""
        # 1ì°¨: ì§ì ‘ ê²€ìƒ‰
        result = await ContentSearcher._search_tmdb_direct(session, name)

        # ê²€ìƒ‰ ì„±ê³µ ì‹œ ë°˜í™˜
        if result[0] is not None:
            return result

        # 2ì°¨: Google ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ ì •ë³´ ì¶”ì¶œ
        print(f"ğŸ” TMDB ì§ì ‘ ê²€ìƒ‰ ì‹¤íŒ¨, Google ìŠ¤í¬ë˜í•‘ ì‹œë„: {name}")
        google_info = await scrape_google_search_info(name, "ì˜í™”")

        if google_info and google_info.get('title'):
            print(f"ğŸ” Googleì—ì„œ ì¶”ì¶œí•œ ì •ë³´ - ì œëª©: {google_info['title']}, ê°ë…: {google_info.get('author')}")
            return google_info['title'], google_info.get('year'), google_info.get('author'), None, 'movie'

        # 3ì°¨: ë²ˆì—­ í›„ ì¬ê²€ìƒ‰
        translated = await translate_to_english(name)
        if translated and translated != name:
            print(f"ğŸ” ë²ˆì—­ëœ ì œëª©ìœ¼ë¡œ TMDB ì¬ê²€ìƒ‰: {translated}")
            result = await ContentSearcher._search_tmdb_direct(session, translated)
            if result[0] is not None:
                return result

        return None, None, None, None, None

    @staticmethod
    async def _search_manga_direct(session, name):
        """MangaDexì—ì„œ ì§ì ‘ ê²€ìƒ‰ (ë‚´ë¶€ìš©)"""
        url = f"https://api.mangadex.org/manga?title={name}&limit=1&includes[]=author&includes[]=cover_art"

        try:
            async with session.get(url) as response:
                data = await response.json()

            if data.get('data') and len(data['data']) > 0:
                manga = data['data'][0]
                attributes = manga.get('attributes', {})
                title_dict = attributes.get('title', {})
                alt_titles = attributes.get('altTitles', [])

                title = None
                # 1. title ê°ì²´ì—ì„œ í•œêµ­ì–´ ì œëª© ì°¾ê¸°
                if 'ko' in title_dict:
                    title = title_dict['ko']

                # 2. altTitlesì—ì„œ í•œêµ­ì–´ ì œëª© ì°¾ê¸°
                if not title:
                    for alt in alt_titles:
                        if 'ko' in alt:
                            title = alt['ko']
                            break

                # 3. title ê°ì²´ì—ì„œ ì˜ì–´ ì œëª© ì°¾ê¸°
                if not title and 'en' in title_dict:
                    title = title_dict['en']

                # 4. ê·¸ë˜ë„ ì—†ìœ¼ë©´ ì²«ë²ˆì§¸ ì œëª© ì‚¬ìš©
                if not title:
                    title = list(title_dict.values())[0] if title_dict else name

                year = str(attributes.get('year')) if attributes.get('year') else None
                author = None
                relationships = manga.get('relationships', [])
                for rel in relationships:
                    if rel.get('type') == 'author':
                        author_attrs = rel.get('attributes', {})
                        author = author_attrs.get('name')
                        if author and not await is_korean(author):
                            author = await translate_to_korean(author)
                        break

                img_url = None
                for rel in relationships:
                    if rel.get('type') == 'cover_art':
                        cover_attrs = rel.get('attributes', {})
                        filename = cover_attrs.get('fileName')
                        if filename:
                            manga_id = manga.get('id')
                            img_url = f"https://uploads.mangadex.org/covers/{manga_id}/{filename}"
                        break

                return title, year, author, img_url

        except Exception as e:
            print(f"âŒ MangaDex API error: {e}")

        return None, None, None, None

    @staticmethod
    async def search_manga(session, name):
        """MangaDexì—ì„œ ë§Œí™” ê²€ìƒ‰ (í•œêµ­ì–´ ì œëª© ì—†ìœ¼ë©´ Google ìŠ¤í¬ë˜í•‘)"""
        original_name = name

        # 1ì°¨: ì˜ì–´ë¡œ ë²ˆì—­ í›„ ê²€ìƒ‰
        translated_name = await translate_to_english(name)
        result = await ContentSearcher._search_manga_direct(session, translated_name)

        # í•œêµ­ì–´ ì œëª©ì´ ìˆìœ¼ë©´ ë°˜í™˜
        if result[0] is not None and await is_korean(result[0]):
            return result

        # 3ì°¨: Google ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ ì •ë³´ ì¶”ì¶œ
        print(f"ğŸ” MangaDexì—ì„œ í•œêµ­ì–´ ì œëª© ëª» ì°¾ìŒ, Google ìŠ¤í¬ë˜í•‘ ì‹œë„: {original_name}")
        google_info = await scrape_google_search_info(original_name, "ë§Œí™”")

        if google_info and google_info.get('title'):
            print(f"ğŸ” Googleì—ì„œ ì¶”ì¶œí•œ ì •ë³´ - ì œëª©: {google_info['title']}, ì‘ê°€: {google_info.get('author')}")
            # ì´ë¯¸ì§€ëŠ” ì›ë˜ MangaDex ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ None
            img_url = result[3] if result else None
            return google_info['title'], google_info.get('year') or result[1], google_info.get('author') or result[2], img_url or result[3]

        print(f"âŒ MangaDex/Google: No results for '{original_name}'")
        return None, None, None, None

    @staticmethod
    async def _search_naver_webtoon(session, name):
        """ë„¤ì´ë²„ ì›¹íˆ°ì—ì„œ ì§ì ‘ ê²€ìƒ‰ (ë‚´ë¶€ìš©)"""
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        try:
            search_url = f"https://comic.naver.com/api/search/all?keyword={name}"
            async with session.get(search_url, headers=headers) as response:
                if response.status == 200:
                    data = await response.json()
                    webtoons = data.get('searchWebtoonResult', {}).get('searchViewList', [])

                    if webtoons:
                        webtoon = webtoons[0]
                        title = webtoon.get('titleName', name)
                        author = webtoon.get('displayAuthor')
                        img_url = webtoon.get('thumbnailUrl')

                        return title, "ë„¤ì´ë²„ì›¹íˆ°", author, img_url
        except Exception as e:
            print(f"âš ï¸ Naver webtoon search failed: {e}")

        return None, None, None, None

    @staticmethod
    async def search_webtoon(session, name):
        """ì›¹íˆ° ê²€ìƒ‰ (ë„¤ì´ë²„ â†’ ì¹´ì¹´ì˜¤ â†’ Google ìŠ¤í¬ë˜í•‘)"""
        # 1ì°¨: ë„¤ì´ë²„ ì›¹íˆ° ê²€ìƒ‰
        result = await ContentSearcher._search_naver_webtoon(session, name)
        if result[0] is not None:
            return result

        # 3ì°¨: Google ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ ì›¹íˆ° ì •ë³´ ì¶”ì¶œ
        print(f"ğŸ” ì›¹íˆ° ì§ì ‘ ê²€ìƒ‰ ì‹¤íŒ¨, Google ìŠ¤í¬ë˜í•‘ ì‹œë„: {name}")
        google_info = await scrape_google_search_info(name, "ì›¹íˆ°")

        if google_info and google_info.get('title'):
            print(f"ğŸ” Googleì—ì„œ ì¶”ì¶œí•œ ì •ë³´ - ì œëª©: {google_info['title']}, ì‘ê°€: {google_info.get('author')}")
            return google_info['title'], "Google ê²€ìƒ‰", google_info.get('author'), None

        print(f"âŒ Webtoon: No results for '{name}'")
        return None, None, None, None
